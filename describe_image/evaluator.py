import json
from typing import Dict, Any, List

from .asr import transcribe_audio
from .grammar import check_grammar
from .fluency import analyze_fluency_and_pronunciation
from .prompts import DESCRIBE_IMAGE_PROMPT_TEMPLATE

class DescribeImageEvaluator:
    """
    Evaluator for PTE Describe Image task.
    Integrates ASR (Whisper), Grammar Check (LanguageTool), 
    Fluency/Pronunciation Analysis (MFA), and Content Evaluation (LLM).
    """

    def evaluate(self, wav_path: str, image_schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run the full evaluation pipeline.
        
        Args:
            wav_path: Path to the audio file.
            image_schema: Dictionary containing 'description', 'key_points', etc.
            
        Returns:
            Dictionary containing the full evaluation report.
        """
        print(f"Step 1: Transcribing audio from {wav_path}...")
        transcript = transcribe_audio(wav_path, model_size="medium")
        print(f"Transcript: {transcript[:50]}...")

        print("Step 2: Checking grammar...")
        grammar_issues = check_grammar(transcript)
        
        print("Step 3: Analyzing fluency and pronunciation (MFA)...")
        fp_results = analyze_fluency_and_pronunciation(wav_path, transcript)
        
        # Extract scores (default to 0 if something went wrong)
        fluency_score = fp_results.get("fluency_score", 0)
        pronunciation_score = fp_results.get("pronunciation_score", 0)
        
        # Content score is initially 0 (to be evaluated by LLM) or could be based on keyword matching
        # For now, we pass 0 as the 'current' content score to the LLM
        content_score = 0
        
        print("Step 4: Generating LLM prompt...")
        prompt = self.construct_prompt(
            transcript=transcript,
            image_schema=image_schema,
            content_score=content_score,
            fluency_score=fluency_score,
            pronunciation_score=pronunciation_score,
            grammar_issues=grammar_issues
        )
        
        print("Step 5: Calling LLM (Simulation)...")
        llm_response = self.call_llm(prompt)
        
        return {
            "transcript": transcript,
            "grammar_issues": grammar_issues,
            "algorithmic_scores": fp_results,
            "llm_evaluation": llm_response,
            "final_scores": {
                "content": llm_response.get("suggested_content_score", content_score),
                "fluency": llm_response.get("suggested_fluency_score", fluency_score),
                "pronunciation": llm_response.get("suggested_pronunciation_score", pronunciation_score)
            },
            "prompt_used": prompt
        }

    def construct_prompt(
        self, 
        transcript: str, 
        image_schema: Dict[str, Any],
        content_score: float,
        fluency_score: float,
        pronunciation_score: float,
        grammar_issues: List[str]
    ) -> str:
        """
        Fill the prompt template with data.
        """
        # Format grammar issues
        grammar_text = "\n ".join([f"- {i}" for i in grammar_issues]) if grammar_issues else "None detected."
        
        # Format image schema as JSON string
        image_json = json.dumps(image_schema, indent=2)
        
        return DESCRIBE_IMAGE_PROMPT_TEMPLATE.format(
            content_score=content_score,
            fluency_score=fluency_score,
            pronunciation_score=pronunciation_score,
            image_schema_json=image_json,
            student_transcript=transcript,
            grammar_issues=grammar_text
        )

    def call_llm(self, prompt: str) -> Dict[str, Any]:
        """
        Placeholder for LLM API call.
        In a real system, this would call OpenAI/Gemini/Anthropic.
        Here, we return a mock response or raise a NotImplementedError 
        if the user wants to implement it themselves.
        """
        # Mock response for demonstration
        return {
            "missing_points": ["This is a mock evaluation.", "LLM API not connected."],
            "irrelevant_content": [],
            "major_grammar_issues": [],
            "suggested_content_score": 50,
            "suggested_fluency_score": 50,
            "suggested_pronunciation_score": 50,
            "overall_assessment": "This is a placeholder response generated by the system because no LLM API is configured."
        }
