I will upgrade the main repository by porting the **data** and **UI architecture** from the reference clone, while ensuring all tasks utilize the **unified scoring engine** you've built in the main repo.

### **Technical Implementation:**

1. **Port Task Data**:

   * Copy the `data_2/` directory (containing images, lectures, and reference JSON files) from the reference clone to the main repository. This provides the content for Describe Image and Retell Lecture.

2. **Integrate Multi-Task UI**:

   * Port the `dashboard.html`, `describe_image.html`, and `retell_lecture.html` templates to [api/templates/](file:///c:/Users/Acer/DataScience/PTE/api/templates/).

   * Update [app.py](file:///c:/Users/Acer/DataScience/PTE/api/app.py) with the new task routes and the **asynchronous Job Queue system**.

3. **Unified Engine Integration**:

   * **Crucial Step**: Adapt all tasks (Repeat Sentence, Describe Image, etc.) to use your existing [validator.py](file:///c:/Users/Acer/DataScience/PTE/api/validator.py) and [pte\_tools.py](file:///c:/Users/Acer/DataScience/PTE/pte_tools.py) as the core "engine."

   * This ensures consistent parallel MFA alignment and advanced phonetic scoring across all task types.

4. **Specific Task Orchestration**:

   * **Repeat Sentence**: Integrate it into the dashboard and ensure it routes through the advanced pronunciation scoring in `pte_core`.

   * **Describe Image/Retell Lecture**: Use the new evaluators but ensure they rely on the main repo's ASR and MFA outputs.

5. **Verification**:

   * Verify the dashboard is fully functional with the new data.

   * Run a full end-to-end test of a Repeat Sentence task using the unified engine.

Does this approach of using your engine while porting the clone's data and UI meet your requirements?
