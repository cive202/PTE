I will modify the system to ensure **all** spoken sounds (including "umm", "ar", or substituted words) are analyzed for phonemes, and provide the detailed comparison you requested.

### 1. Handling "ar..." and "umm" (The "Not Getting Phonemes" Fix)
Currently, the system **only** checks phonemes for words that match the text correctly. It ignores "Inserted" words (like "umm" or substitutions).
*   **Fix**: I will update the validator to run **Wav2vec2** on **Inserted** words too.
*   **Result**: If you say "The **umm** dog", and the ASR catches "umm", we will now show you the phonemes for "umm" (e.g., `AH M`) in the JSON, instead of ignoring it. This is **not hardcoded**; it works for any inserted sound the ASR detects.

### 2. Detailed Phoneme Alignment
I will implement a "diff" for phonemes to show exactly what went wrong.
*   **New Function**: `analyze_phoneme_errors` in `pte_core`.
*   **Logic**: It will align the "Expected" vs "Observed" phonemes.
    *   *Match*: `K` vs `K`
    *   *Substitution*: Expected `T`, Got `D`
    *   *Insertion*: Got `S` (extra)
    *   *Deletion*: Expected `Z` (missing)

### 3. Enhanced JSON Output
I will update `api/validator.py` to include:
*   `phoneme_analysis`: The detailed alignment above.
*   `mfa_timings`: Precise start/end times for each phoneme (for correct words).
*   `observed_phones`: Now included for **Inserted** words too.

This ensures you see phonemes for everything you say, and get detailed timing/stress info.